#!/usr/bin/env python3
"""
Gemma-3 LoRA Fine-tuning for Hearthstone AI.

Uses mlx-lm for efficient LoRA training on Apple Silicon.
Trains on text prompts generated by convert_to_prompts.py.
"""

import sys
import os
import argparse
from pathlib import Path

# Check for mlx-lm
try:
    from mlx_lm import load, generate
    from mlx_lm.tuner import train as lora_train
    from mlx_lm.tuner.trainer import TrainingArgs
    MLX_LM_AVAILABLE = True
except ImportError:
    MLX_LM_AVAILABLE = False
    print("mlx-lm not installed. Install with: pip install mlx-lm")


def check_dependencies():
    """Check if required dependencies are available."""
    if not MLX_LM_AVAILABLE:
        print("\nError: mlx-lm is required for Gemma training.")
        print("Install with: pip install mlx-lm")
        sys.exit(1)


def train_lora(
    data_path: str,
    output_dir: str = "models/gemma_lora",
    model_name: str = "mlx-community/gemma-3-1b-it-4bit",
    epochs: int = 3,
    batch_size: int = 4,
    learning_rate: float = 1e-4,
    lora_rank: int = 8,
    lora_layers: int = 16,
    val_split: float = 0.1,
    max_seq_length: int = 512,
):
    """
    Fine-tune Gemma with LoRA adapters.
    
    Args:
        data_path: Path to JSONL training data
        output_dir: Where to save LoRA adapters
        model_name: Hugging Face model ID
        epochs: Number of training epochs
        batch_size: Training batch size
        learning_rate: Learning rate
        lora_rank: LoRA rank (lower = fewer params, higher = more capacity)
        lora_layers: Number of layers to apply LoRA to
        val_split: Validation split ratio
        max_seq_length: Maximum sequence length for training
    """
    check_dependencies()
    
    print(f"\n{'='*60}")
    print("Gemma LoRA Fine-tuning for Hearthstone")
    print(f"{'='*60}")
    print(f"Model: {model_name}")
    print(f"Data: {data_path}")
    print(f"Output: {output_dir}")
    print(f"Epochs: {epochs}")
    print(f"Batch Size: {batch_size}")
    print(f"Learning Rate: {learning_rate}")
    print(f"LoRA Rank: {lora_rank}")
    print(f"LoRA Layers: {lora_layers}")
    print(f"{'='*60}\n")
    
    # Check data exists
    if not os.path.exists(data_path):
        print(f"Error: Training data not found at {data_path}")
        print("Generate with: python scripts/convert_to_prompts.py")
        sys.exit(1)
    
    # Count samples
    with open(data_path) as f:
        num_samples = sum(1 for _ in f)
    print(f"Found {num_samples} training samples")
    
    # Create output directory
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Set up training arguments
    training_args = TrainingArgs(
        batch_size=batch_size,
        iters=epochs * (num_samples // batch_size),  # Convert epochs to iterations
        val_batches=max(1, int(num_samples * val_split / batch_size / 10)),
        steps_per_report=50,
        steps_per_eval=200,
        save_every=500,
        adapter_path=output_dir,
        max_seq_length=max_seq_length,
        learning_rate=learning_rate,
    )
    
    # LoRA config
    lora_config = {
        "rank": lora_rank,
        "alpha": lora_rank * 2,  # Standard alpha = 2 * rank
        "dropout": 0.05,
        "scale": 1.0,
    }
    
    print("Loading model...")
    model, tokenizer = load(model_name)
    
    print("Starting LoRA training...")
    try:
        lora_train(
            model=model,
            tokenizer=tokenizer,
            args=training_args,
            train_dataset=data_path,
            lora_config=lora_config,
            lora_layers=lora_layers,
        )
        print(f"\nTraining complete! Adapters saved to: {output_dir}")
        
    except Exception as e:
        print(f"\nTraining error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def test_inference(adapter_path: str, model_name: str = "mlx-community/gemma-3-1b-it-4bit"):
    """Test inference with trained adapters."""
    check_dependencies()
    
    print(f"Loading model with adapters from {adapter_path}...")
    model, tokenizer = load(model_name, adapter_path=adapter_path)
    
    # Test prompt
    test_prompt = """<bos>Game State:
My Board: empty
My Hand: Fireball(4), Frostbolt(2), Azure Drake(5/4/4)
Enemy Board: Knife Juggler(2/2/2)

Best Action:"""
    
    print("\n--- Test Prompt ---")
    print(test_prompt)
    print("\n--- Generated Response ---")
    
    response = generate(
        model,
        tokenizer,
        prompt=test_prompt,
        max_tokens=50,
        temp=0.1,
    )
    print(response)


def main():
    parser = argparse.ArgumentParser(description='Gemma LoRA Fine-tuning')
    subparsers = parser.add_subparsers(dest='command', help='Commands')
    
    # Train command
    train_parser = subparsers.add_parser('train', help='Train LoRA adapters')
    train_parser.add_argument('--data', type=str, default='data/gemma_train.jsonl',
                              help='Training data JSONL')
    train_parser.add_argument('--output', type=str, default='models/gemma_lora',
                              help='Output directory for adapters')
    train_parser.add_argument('--model', type=str, default='mlx-community/gemma-3-1b-it-4bit',
                              help='Base model to fine-tune')
    train_parser.add_argument('--epochs', type=int, default=3, help='Training epochs')
    train_parser.add_argument('--batch-size', type=int, default=4, help='Batch size')
    train_parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
    train_parser.add_argument('--lora-rank', type=int, default=8, help='LoRA rank')
    train_parser.add_argument('--lora-layers', type=int, default=16, help='LoRA layers')
    train_parser.add_argument('--max-seq-len', type=int, default=512, help='Max sequence length')
    
    # Test command
    test_parser = subparsers.add_parser('test', help='Test trained model')
    test_parser.add_argument('--adapters', type=str, default='models/gemma_lora',
                             help='Path to LoRA adapters')
    test_parser.add_argument('--model', type=str, default='mlx-community/gemma-3-1b-it-4bit',
                             help='Base model')
    
    args = parser.parse_args()
    
    if args.command == 'train':
        train_lora(
            data_path=args.data,
            output_dir=args.output,
            model_name=args.model,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.lr,
            lora_rank=args.lora_rank,
            lora_layers=args.lora_layers,
            max_seq_length=args.max_seq_len,
        )
    elif args.command == 'test':
        test_inference(args.adapters, args.model)
    else:
        parser.print_help()
        print("\nQuick Start:")
        print("  1. Generate training data:")
        print("     python scripts/convert_to_prompts.py")
        print("  2. Train LoRA adapters:")
        print("     python training/gemma_trainer.py train --data data/gemma_train.jsonl")
        print("  3. Test the model:")
        print("     python training/gemma_trainer.py test")


if __name__ == '__main__':
    main()
