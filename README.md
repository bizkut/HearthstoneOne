# ğŸƒ HearthstoneOne

> **AI Assistant for Hearthstone** â€” Real-time coaching + AlphaZero training + HSTracker integration

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![CUDA](https://img.shields.io/badge/CUDA-12.2-76B900?style=for-the-badge&logo=nvidia&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ§  **AlphaZero AI** | Self-play training with MCTS + Deep Learning |
| ğŸ¤– **Transformer Model** | Attention-based architecture for card synergies |
| ğŸ‘ï¸ **Real-Time Overlay** | Arrow indicators for suggested plays |
| ğŸ”Œ **HSTracker Integration** | WebSocket bridge for seamless connectivity |
| ğŸ¯ **Mulligan Assistant** | Learned policy for keep/replace decisions |
| ğŸ“Š **HSReplay Training** | Learn from human games via behavior cloning |

---

## ğŸ³ Quick Start (Docker)

### Prerequisites
- [Docker](https://docs.docker.com/get-docker/)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) (for GPU)

### 1. Start the WebSocket Server
```bash
# Start inference server with GPU
docker compose up -d

# Check logs
docker compose logs -f server
```

The server runs on `ws://localhost:9876` and connects to HSTracker.

### 2. Train the AI

**Option A: Self-Play (MLP Model)**
```bash
docker compose run train
```

**Option B: Imitation Learning (Transformer)**
```bash
# Step 1: Parse HSReplay files
mkdir -p data/replays
# Copy your .xml replay files to data/replays/

docker compose run parser

# Step 2: Train on parsed data
docker compose run imitation
```

### 3. Stop Everything
```bash
docker compose down
```

---

## ğŸ’» Local Development

### Installation
```bash
# Clone
git clone https://github.com/Kevzi/-HearthstoneOne.git
cd HearthstoneOne

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```

### Run WebSocket Server
```bash
python runtime/websocket_server.py --host localhost --port 9876 --model models/best_model.pt
```

### Train Models
```bash
# MLP (AlphaZero self-play)
python training/trainer.py --epochs 100 --output models/

# Transformer (behavior cloning)
python training/imitation_trainer.py --data data/replays.json --epochs 50 --output models/transformer_model.pt

# Test with dummy data
python training/imitation_trainer.py --dummy --epochs 10
```

---

## ğŸ”— HSTracker Integration

HearthstoneOne integrates with [HSTracker](https://github.com/HearthSim/HSTracker) via WebSocket:

1. **Start the Python server** (Docker or local)
2. **HSTracker connects automatically** and streams Power.log
3. **AI suggestions appear** in the overlay with arrow indicators

### Message Protocol
```javascript
// Client â†’ Server
{ "type": "log", "line": "..." }
{ "type": "request_suggestion" }
{ "type": "request_mulligan", "hand_cards": [...], "opponent_class": 2 }

// Server â†’ Client
{ "type": "suggestion", "action": "play_card", "card_id": "...", "win_probability": 0.65 }
{ "type": "mulligan", "keep_probabilities": [0.9, 0.2, 0.8] }
```

---

## ğŸ“ Project Structure

```
HearthstoneOne/
â”œâ”€â”€ ai/                        # ğŸ§  AI Models
â”‚   â”œâ”€â”€ model.py               # MLP policy/value network
â”‚   â”œâ”€â”€ transformer_model.py   # Transformer with self-attention
â”‚   â”œâ”€â”€ mcts.py                # Monte Carlo Tree Search
â”‚   â”œâ”€â”€ encoder.py             # State encoding (690 dims)
â”‚   â”œâ”€â”€ mulligan_policy.py     # Mulligan decision network
â”‚   â””â”€â”€ game_wrapper.py        # Simulator interface
â”‚
â”œâ”€â”€ training/                  # ğŸ‹ï¸ Training Scripts
â”‚   â”œâ”€â”€ trainer.py             # AlphaZero self-play
â”‚   â”œâ”€â”€ imitation_trainer.py   # Behavior cloning
â”‚   â”œâ”€â”€ replay_parser.py       # HSReplay XML parser
â”‚   â””â”€â”€ mulligan_trainer.py    # Mulligan policy training
â”‚
â”œâ”€â”€ runtime/                   # ğŸ”Œ Runtime Services
â”‚   â”œâ”€â”€ websocket_server.py    # WebSocket API
â”‚   â”œâ”€â”€ parser.py              # Power.log parser
â”‚   â””â”€â”€ log_watcher.py         # File watcher
â”‚
â”œâ”€â”€ simulator/                 # ğŸ® Game Engine
â”‚   â”œâ”€â”€ game.py                # Game state
â”‚   â”œâ”€â”€ player.py              # Player logic
â”‚   â””â”€â”€ entities.py            # Cards, Minions, Heroes
â”‚
â”œâ”€â”€ HSTracker/                 # ğŸ“± Swift Client (macOS)
â”‚   â””â”€â”€ HearthstoneOne/        # WebSocket client + overlay
â”‚
â”œâ”€â”€ Dockerfile                 # ğŸ³ CUDA 12.2 container
â”œâ”€â”€ docker-compose.yml         # Multi-service orchestration
â””â”€â”€ requirements.txt           # Python dependencies
```

---

## ğŸ§  Model Architectures

### MLP (HearthstoneModel)
- Input: 690-dimensional game state vector
- Hidden: 512 â†’ 256 neurons
- Output: Policy (action probs) + Value (-1 to +1)

### Transformer (CardTransformer)
- Input: Sequence of card embeddings
- 4 attention layers, 4 heads, 128 hidden dim
- Self-attention learns card relationships
- ~1M parameters, fast inference on Pascal GPUs

---

## ğŸ–¥ï¸ Hardware Compatibility

| GPU | Training | Inference |
|-----|----------|-----------|
| **Pascal (GTX 1080)** | âœ… | âœ… Fast |
| **Turing (RTX 2080)** | âœ… | âœ… Fast |
| **Ampere (RTX 3090)** | âœ… | âœ… Very Fast |
| **Apple Silicon (MPS)** | âœ… (with fallback) | âœ… |
| **CPU** | âœ… Slow | âœ… |

---

## ğŸ“œ License

MIT License â€” See [LICENSE](LICENSE)

---

<p align="center">
  <b>HearthstoneOne</b> â€” Open-source AI for research and education.
</p>
