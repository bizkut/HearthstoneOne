# ğŸƒ HearthstoneOne

> **AI Assistant for Hearthstone** â€” Real-time coaching + AlphaZero training + HSTracker integration

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![CUDA](https://img.shields.io/badge/CUDA-12.2-76B900?style=for-the-badge&logo=nvidia&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ§  **AlphaZero AI** | Self-play training with MCTS + Deep Learning |
| ğŸ¤– **Transformer Model** | Attention-based architecture for card synergies |
| ğŸ‘ï¸ **Real-Time Overlay** | Arrow indicators for suggested plays |
| ğŸ”Œ **HSTracker Integration** | WebSocket bridge for seamless connectivity |
| ğŸ¯ **Mulligan Assistant** | Learned policy for keep/replace decisions |
| ğŸ“Š **HSReplay Training** | Learn from human games via behavior cloning |

---

## ğŸ³ Quick Start (Docker)

### Prerequisites
- [Docker](https://docs.docker.com/get-docker/)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) (for GPU)

### 1. Start the WebSocket Server
```bash
# Start inference server with GPU
docker compose up -d

# Check logs
docker compose logs -f server
```

The server runs on `ws://localhost:9876` and connects to HSTracker.

### 2. Train the AI

**Option A: Self-Play (MLP Model)**
```bash
docker compose run train
```

**Option B: Imitation Learning (Transformer)**
```bash
# Step 1: Parse HSReplay files
mkdir -p data/replays
# Copy your .xml replay files to data/replays/

docker compose run parser

# Step 2: Train on parsed data
docker compose run imitation
```

### 3. Stop Everything
```bash
docker compose down
```

---

## ğŸ’» Local Development

### Installation
```bash
# Clone
git clone https://github.com/bizkut/HearthstoneOne.git
cd HearthstoneOne

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```

### Run WebSocket Server
```bash
python runtime/websocket_server.py --host localhost --port 9876 --model models/best_model.pt
```

### Train Models (Complete Pipeline)

Building the AI model follows a multi-phase approach. **Follow these steps in order:**

---

#### Step 0: Prerequisites
```bash
# Install dependencies
pip install -r requirements.txt

# Scrape meta decks (first time only, creates data/meta_deck_lists.json)
python3 scripts/scrape_top_decks.py
```

---

#### Step 1: Generate Training Data (Heuristic)
> Fast data generation using rule-based play. Good for initial training.

```bash
python3 scripts/generate_self_play.py --num-games 10000 --output data/self_play_data.json
```
*Generates ~30k samples per 1000 games. Expect ~10 min for 10k games.*

---

#### Step 2: Train the Model (Imitation Learning)
> Supervised learning on the generated data.

```bash
# Basic training (CPU/MPS)
python3 training/imitation_trainer.py \
    --data data/self_play_data.json \
    --epochs 50 \
    --batch-size 64

# CUDA GPU Training (Recommended)
python3 training/imitation_trainer.py \
    --data data/self_play_data.json \
    --epochs 200 \
    --batch-size 4096 \
    --lr 1e-3 \
    --xlarge \
    --gpu-cache
```

| Model Size | Flag | Hidden | Layers | Params | VRAM |
|------------|------|--------|--------|--------|------|
| Default | - | 128 | 4 | ~1M | 4GB |
| Large | `--large` | 256 | 6 | ~5.5M | 8GB |
| XLarge | `--xlarge` | 512 | 8 | ~12M | 16GB |

---

#### Step 3: Generate RL Data (Neural Network)
> After training, generate higher quality data using the trained model.

**Option A: Policy Network (Fast)**
```bash
python3 scripts/generate_alphazero_play.py \
    --num-games 5000 \
    --model models/transformer_model.pt \
    --output data/rl_data.json
```
*Uses the Neural Network directly to select actions. Fast but limited lookahead.*

**Option B: Full MCTS (Highest Quality)**
```bash
python3 scripts/generate_mcts_play.py \
    --sims 50 \
    --games 1000 \
    --model models/transformer_model.pt \
    --output data/mcts_data.json
```
*Uses Monte Carlo Tree Search with the model. Slower but produces stronger play.*

| Script | Method | Speed | Quality |
|--------|--------|-------|---------|
| `generate_alphazero_play.py` | Policy Network | âš¡ Fast | Good |
| `generate_mcts_play.py` | MCTS (50 sims) | ğŸ¢ Slow | Best |

---

#### Step 4: Iterate (AlphaZero Loop)
Repeat Steps 2-3 to improve the model:
1. Train on combined data
2. Generate new RL data with the improved model
3. Repeat until performance plateaus

```bash
# Merge datasets
python3 -c "import json; d1=json.load(open('data/self_play_data.json')); d2=json.load(open('data/rl_data.json')); d1['samples'].extend(d2['samples']); json.dump(d1, open('data/combined.json','w'))"

# Train on combined data
python3 training/imitation_trainer.py --data data/combined.json --epochs 100 --xlarge
```

---

#### Which Script Should I Use?

| Phase | Script | When to Use |
|-------|--------|-------------|
| **Bootstrap** | `generate_self_play.py` | First-time training, no model yet |
| **Iteration Loop** | `generate_alphazero_play.py` | Continuous improvement (fast) |
| **Final Polish** | `generate_mcts_play.py` | Best quality before deployment (slow) |

> [!TIP]
> For practical training, use `generate_alphazero_play.py` in your iteration loop. Save `generate_mcts_play.py` for the final training run when you want maximum quality.

---

#### Apple Silicon (MLX) Training
For M1/M2/M3/M4 Macs â€” **10x faster** than PyTorch on Apple Silicon:

```bash
# Same interface as PyTorch trainer - conversions happen automatically
python3 training/mlx_imitation_trainer.py \
    --data data/self_play_data.json \
    --epochs 100 \
    --batch-size 1024 \
    --large
```

**What happens automatically:**
1. JSON â†’ Binary cache (on first run, reused afterward)
2. MLX training on Unified Memory
3. Output â†’ PyTorch `.pt` model

| Model Size | Flag | Dataset Size | Speed |
|------------|------|--------------|-------|
| Default | - | <50k samples | ~5s/epoch |
| Large | `--large` | 50k-500k | ~8s/epoch |
| XLarge | `--xlarge` | 500k+ | ~15s/epoch |

---

### Legacy MLP Training
```bash
python training/trainer.py --epochs 100 --output models/
```

---



## ğŸ”— HSTracker Integration

HearthstoneOne integrates with [HSTracker](https://github.com/HearthSim/HSTracker) via WebSocket:

1. **Start the Python server** (Docker or local)
2. **HSTracker connects automatically** and streams Power.log
3. **AI suggestions appear** in the overlay with arrow indicators

### Message Protocol
```javascript
// Client â†’ Server
{ "type": "log", "line": "..." }
{ "type": "request_suggestion" }
{ "type": "request_mulligan", "hand_cards": [...], "opponent_class": 2 }

// Server â†’ Client
{ "type": "suggestion", "action": "play_card", "card_id": "...", "win_probability": 0.65 }
{ "type": "mulligan", "keep_probabilities": [0.9, 0.2, 0.8] }
```

---

## ğŸ“ Project Structure

```
HearthstoneOne/
â”œâ”€â”€ ai/                        # ğŸ§  AI Models
â”‚   â”œâ”€â”€ transformer_model.py   # CardTransformer (Main Model)
â”‚   â”œâ”€â”€ mcts.py                # Monte Carlo Tree Search
â”‚   â”œâ”€â”€ opponent_model.py      # Opponent Modeling (Phase 8)
â”‚   â”œâ”€â”€ deck_classifier.py     # Archetype Detection
â”‚   â”œâ”€â”€ encoder.py             # State Encoding
â”‚   â””â”€â”€ game_wrapper.py        # Simulator Interface
â”‚
â”œâ”€â”€ training/                  # ğŸ‹ï¸ Training Scripts
â”‚   â”œâ”€â”€ imitation_trainer.py   # Transformer Trainer
â”‚   â””â”€â”€ trainer.py             # Legacy AlphaZero Trainer
â”‚
â”œâ”€â”€ scripts/                   # ğŸ› ï¸ Utility Scripts
â”‚   â”œâ”€â”€ generate_self_play.py  # Heuristic Data Generator (Step 1)
â”‚   â”œâ”€â”€ generate_mcts_play.py  # MCTS Data Generator (Step 3)
â”‚   â”œâ”€â”€ scrape_top_decks.py    # Meta Deck Scraper
â”‚   â””â”€â”€ convert_to_binary.py   # Binary Format Converter (MLX)
â”‚
â”œâ”€â”€ runtime/                   # ğŸ”Œ Runtime Services
â”‚   â”œâ”€â”€ websocket_server.py    # WebSocket API
â”‚   â”œâ”€â”€ parser.py              # Power.log parser
â”‚   â””â”€â”€ log_watcher.py         # File watcher
â”‚
â”œâ”€â”€ simulator/                 # ğŸ® Game Engine
â”‚   â”œâ”€â”€ game.py                # Game state
â”‚   â”œâ”€â”€ player.py              # Player logic
â”‚   â””â”€â”€ entities.py            # Cards, Minions, Heroes
â”‚
â”œâ”€â”€ HSTracker/                 # ğŸ“± Swift Client (macOS)
â”‚   â””â”€â”€ HearthstoneOne/        # WebSocket client + overlay
â”‚
â”œâ”€â”€ Dockerfile                 # ğŸ³ CUDA 12.2 container
â”œâ”€â”€ docker-compose.yml         # Multi-service orchestration
â””â”€â”€ requirements.txt           # Python dependencies
```

---

## ğŸ§  Model Architectures

### MLP (HearthstoneModel)
- Input: 690-dimensional game state vector
- Hidden: 512 â†’ 256 neurons
- Output: Policy (action probs) + Value (-1 to +1)

### Transformer (CardTransformer)
- Input: Sequence of card embeddings
- 4 attention layers, 4 heads, 128 hidden dim
- Self-attention learns card relationships
- ~1M parameters, fast inference on Pascal GPUs

---

## ğŸ§  Training (Apple Silicon Optimized)

This project features a highly optimized training pipeline designed for Apple Silicon (M1/M2/M3/M4) chips, utilizing the **MLX** framework and **Unified Memory**.

### Phase 1: Imitation Learning (Warm Start)
Train the model on massive datasets (e.g., 2.7M+ samples) without RAM bottlenecks using disk-based streaming.

1.  **Prepare Data (Convert to Binary Memmap):**
    ```bash
    python3 scripts/convert_to_binary.py --input data/self_play_data2.json --output data/binary_data_large
    ```
2.  **Train with MLX (M4 Pro Optimized):**
    ```bash
    python3 training/mlx_imitation_trainer.py --data data/binary_data_large --epochs 100 --batch-size 1024 --large --lr 5e-4
    ```
    *Performance: ~2s per epoch on M4 Pro (25k subset), handles 50GB+ datasets on 24GB RAM.*

### Phase 2: AlphaZero Self-Play (RL)
Improve the model by having it play against itself and learn from the outcomes.

1.  **Convert MLX Model to PyTorch (for Inference):**
    ```bash
    python3 scripts/convert_mlx_to_pt.py --mlx models/mlx_model.npz --pt models/transformer_model.pt --large
    ```
2.  **Generate Self-Play Games:**
    ```bash
    python3 scripts/generate_alphazero_play.py --num-games 5000 --output data/rl_gen_1.json --model models/transformer_model.pt
    ```
3.  **Loop:** Convert new data -> Train (Phase 1) -> Generate -> Repeat.

### â˜ï¸ Hugging Face Integration
Share your massive datasets with the community in optimized Parquet format.
```bash
python3 scripts/push_to_huggingface.py --input data/self_play_data2.json --repo your-username/hearthstone-replays
```

## ğŸ›  Installation

| GPU | Training | Inference |
|-----|----------|-----------|
| **Pascal (GTX 1080)** | âœ… | âœ… Fast |
| **Turing (RTX 2080)** | âœ… | âœ… Fast |
| **Ampere (RTX 3090)** | âœ… | âœ… Very Fast |
| **Apple Silicon (MPS)** | âœ… (with fallback) | âœ… |
| **CPU** | âœ… Slow | âœ… |

---

## ğŸ“œ License

MIT License â€” See [LICENSE](LICENSE)

---

<p align="center">
  <b>HearthstoneOne</b> â€” Open-source AI for research and education.
</p>
