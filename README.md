# ğŸƒ HearthstoneOne

> **AI Assistant for Hearthstone** â€” Real-time coaching + AlphaZero training + HSTracker integration

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![CUDA](https://img.shields.io/badge/CUDA-12.2-76B900?style=for-the-badge&logo=nvidia&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ§  **AlphaZero AI** | Self-play training with MCTS + Deep Learning |
| ğŸ¤– **Transformer Model** | Attention-based architecture for card synergies |
| ğŸ‘ï¸ **Real-Time Overlay** | Arrow indicators for suggested plays |
| ğŸ”Œ **HSTracker Integration** | WebSocket bridge for seamless connectivity |
| ğŸ¯ **Mulligan Assistant** | Learned policy for keep/replace decisions |
| ğŸ“Š **HSReplay Training** | Learn from human games via behavior cloning |

---

## ğŸ³ Quick Start (Docker)

### Prerequisites
- [Docker](https://docs.docker.com/get-docker/)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) (for GPU)

### 1. Start the WebSocket Server
```bash
# Start inference server with GPU
docker compose up -d

# Check logs
docker compose logs -f server
```

The server runs on `ws://localhost:9876` and connects to HSTracker.

### 2. Train the AI

**Option A: Self-Play (MLP Model)**
```bash
docker compose run train
```

**Option B: Imitation Learning (Transformer)**
```bash
# Step 1: Parse HSReplay files
mkdir -p data/replays
# Copy your .xml replay files to data/replays/

docker compose run parser

# Step 2: Train on parsed data
docker compose run imitation
```

### 3. Stop Everything
```bash
docker compose down
```

---

## ğŸ’» Local Development

### Installation
```bash
# Clone
git clone https://github.com/Kevzi/-HearthstoneOne.git
cd HearthstoneOne

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```

### Run WebSocket Server
```bash
python runtime/websocket_server.py --host localhost --port 9876 --model models/best_model.pt
```

### Train Models (Transformer Pipeline)

#### Quick Start
```bash
# 1. Scrape meta decks (first time only)
python3 scripts/scrape_top_decks.py

# 2. Generate training data (~2 min for 1000 games)
python3 scripts/generate_self_play.py --num-games 1000 --output data/self_play_data.json

# 3. Train the model (~6 min for 50 epochs on MPS)
python3 training/imitation_trainer.py --data data/self_play_data.json --epochs 50 --batch-size 64
```

#### Action Space
The model learns to predict which action to take:
| Label | Action |
|-------|--------|
| 0-9 | Play card from hand (index 0-9) |
| 10 | Use Hero Power |
| 11-17 | Attack with minion (board index 0-6) |
| 18 | Attack with hero (weapon) |

#### Expected Results
| Dataset Size | Epochs | Accuracy |
|--------------|--------|----------|
| 1,000 games | 50 | ~75% |
| 5,000 games | 100 | ~80%+ |
| 10,000 games | 100 | ~85%+ |

#### Advanced Training
```bash
# Generate larger dataset for better accuracy
python3 scripts/generate_self_play.py --num-games 10000 --output data/self_play_data.json

# Train with more epochs
python3 training/imitation_trainer.py --data data/self_play_data.json --epochs 100 --batch-size 128 --lr 5e-5
```

### ğŸš€ CUDA Server Training (Recommended)

For best results, train on a CUDA GPU server:

```bash
# One-liner for CUDA training
./scripts/train_cuda.sh

# Or with custom settings
NUM_GAMES=20000 BATCH_SIZE=512 MODEL_SIZE=xlarge ./scripts/train_cuda.sh
```

#### Model Sizes
| Flag | Hidden | Layers | Params | VRAM | Best For |
|------|--------|--------|--------|------|----------|
| (default) | 128 | 4 | ~1M | 4GB | Testing |
| `--large` | 256 | 6 | ~5.5M | 8GB | Good quality |
| `--xlarge` | 512 | 8 | ~12M | 16GB+ | Best quality |

#### CUDA Training Command
```bash
# Generate 20k games (~10 min)
python3 scripts/generate_self_play.py --num-games 20000 --output data/self_play_data.json

# Train XL model (~2 hours on RTX 3090)
python3 training/imitation_trainer.py \
    --data data/self_play_data.json \
    --epochs 200 \
    --batch-size 512 \
    --lr 5e-4 \
    --xlarge
```

### Legacy MLP Training
```bash
python training/trainer.py --epochs 100 --output models/
```

---

## ğŸ”— HSTracker Integration

HearthstoneOne integrates with [HSTracker](https://github.com/HearthSim/HSTracker) via WebSocket:

1. **Start the Python server** (Docker or local)
2. **HSTracker connects automatically** and streams Power.log
3. **AI suggestions appear** in the overlay with arrow indicators

### Message Protocol
```javascript
// Client â†’ Server
{ "type": "log", "line": "..." }
{ "type": "request_suggestion" }
{ "type": "request_mulligan", "hand_cards": [...], "opponent_class": 2 }

// Server â†’ Client
{ "type": "suggestion", "action": "play_card", "card_id": "...", "win_probability": 0.65 }
{ "type": "mulligan", "keep_probabilities": [0.9, 0.2, 0.8] }
```

---

## ğŸ“ Project Structure

```
HearthstoneOne/
â”œâ”€â”€ ai/                        # ğŸ§  AI Models
â”‚   â”œâ”€â”€ model.py               # MLP policy/value network
â”‚   â”œâ”€â”€ transformer_model.py   # Transformer with self-attention
â”‚   â”œâ”€â”€ mcts.py                # Monte Carlo Tree Search
â”‚   â”œâ”€â”€ encoder.py             # State encoding (690 dims)
â”‚   â”œâ”€â”€ mulligan_policy.py     # Mulligan decision network
â”‚   â””â”€â”€ game_wrapper.py        # Simulator interface
â”‚
â”œâ”€â”€ training/                  # ğŸ‹ï¸ Training Scripts
â”‚   â”œâ”€â”€ imitation_trainer.py   # Transformer Trainer
â”‚   â””â”€â”€ trainer.py             # Legacy AlphaZero Trainer
â”‚
â”œâ”€â”€ scripts/                   # ğŸ› ï¸ Utility Scripts
â”‚   â”œâ”€â”€ generate_self_play.py  # Data Generator
â”‚   â”œâ”€â”€ scrape_top_decks.py    # Deck Scraper
â”‚   â””â”€â”€ fetch_meta_decks.py    # Archetype Fetcher
â”‚
â”œâ”€â”€ runtime/                   # ğŸ”Œ Runtime Services
â”‚   â”œâ”€â”€ websocket_server.py    # WebSocket API
â”‚   â”œâ”€â”€ parser.py              # Power.log parser
â”‚   â””â”€â”€ log_watcher.py         # File watcher
â”‚
â”œâ”€â”€ simulator/                 # ğŸ® Game Engine
â”‚   â”œâ”€â”€ game.py                # Game state
â”‚   â”œâ”€â”€ player.py              # Player logic
â”‚   â””â”€â”€ entities.py            # Cards, Minions, Heroes
â”‚
â”œâ”€â”€ HSTracker/                 # ğŸ“± Swift Client (macOS)
â”‚   â””â”€â”€ HearthstoneOne/        # WebSocket client + overlay
â”‚
â”œâ”€â”€ Dockerfile                 # ğŸ³ CUDA 12.2 container
â”œâ”€â”€ docker-compose.yml         # Multi-service orchestration
â””â”€â”€ requirements.txt           # Python dependencies
```

---

## ğŸ§  Model Architectures

### MLP (HearthstoneModel)
- Input: 690-dimensional game state vector
- Hidden: 512 â†’ 256 neurons
- Output: Policy (action probs) + Value (-1 to +1)

### Transformer (CardTransformer)
- Input: Sequence of card embeddings
- 4 attention layers, 4 heads, 128 hidden dim
- Self-attention learns card relationships
- ~1M parameters, fast inference on Pascal GPUs

---

## ğŸ§  Training (Apple Silicon Optimized)

This project features a highly optimized training pipeline designed for Apple Silicon (M1/M2/M3/M4) chips, utilizing the **MLX** framework and **Unified Memory**.

### Phase 1: Imitation Learning (Warm Start)
Train the model on massive datasets (e.g., 2.7M+ samples) without RAM bottlenecks using disk-based streaming.

1.  **Prepare Data (Convert to Binary Memmap):**
    ```bash
    python3 scripts/convert_to_binary.py --input data/self_play_data2.json --output data/binary_data_large
    ```
2.  **Train with MLX (M4 Pro Optimized):**
    ```bash
    python3 training/mlx_imitation_trainer.py --data data/binary_data_large --epochs 100 --batch-size 1024 --large --lr 5e-4
    ```
    *Performance: ~2s per epoch on M4 Pro (25k subset), handles 50GB+ datasets on 24GB RAM.*

### Phase 2: AlphaZero Self-Play (RL)
Improve the model by having it play against itself and learn from the outcomes.

1.  **Convert MLX Model to PyTorch (for Inference):**
    ```bash
    python3 scripts/convert_mlx_to_pt.py --mlx models/mlx_model.npz --pt models/transformer_model.pt --large
    ```
2.  **Generate Self-Play Games:**
    ```bash
    python3 scripts/generate_alphazero_play.py --num-games 5000 --output data/rl_gen_1.json --model models/transformer_model.pt
    ```
3.  **Loop:** Convert new data -> Train (Phase 1) -> Generate -> Repeat.

### â˜ï¸ Hugging Face Integration
Share your massive datasets with the community in optimized Parquet format.
```bash
python3 scripts/push_to_huggingface.py --input data/self_play_data2.json --repo your-username/hearthstone-replays
```

## ğŸ›  Installation

| GPU | Training | Inference |
|-----|----------|-----------|
| **Pascal (GTX 1080)** | âœ… | âœ… Fast |
| **Turing (RTX 2080)** | âœ… | âœ… Fast |
| **Ampere (RTX 3090)** | âœ… | âœ… Very Fast |
| **Apple Silicon (MPS)** | âœ… (with fallback) | âœ… |
| **CPU** | âœ… Slow | âœ… |

---

## ğŸ“œ License

MIT License â€” See [LICENSE](LICENSE)

---

<p align="center">
  <b>HearthstoneOne</b> â€” Open-source AI for research and education.
</p>
