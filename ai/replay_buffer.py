"""
Replay Buffer for HearthstoneOne AI training.

Stores game trajectories generated by self-play to be used for training the neural network.
Follows the AlphaZero pattern:
- Input: Game State (Encoded)
- Targets:
    - Policy: MCTS visit count distribution
    - Value: Final game outcome (+1 for win, -1 for loss)
"""

import numpy as np
import torch
from typing import List, Tuple, Deque
from collections import deque
import random

class ReplayBuffer:
    """Fixed-size buffer to store experience tuples."""
    
    def __init__(self, capacity: int = 10000):
        self.capacity = capacity
        # We store tuples of (state_tensor, policy_probs, value_outcome)
        self.buffer: Deque[Tuple[torch.Tensor, np.ndarray, float]] = deque(maxlen=capacity)
        
    def add(self, state: torch.Tensor, policy_probs: np.ndarray, value: float):
        """Add a single step to the buffer."""
        # Ensure state is detached/cpu if it came from GPU
        if isinstance(state, torch.Tensor):
            state = state.detach().cpu()
            
        self.buffer.append((state, policy_probs, value))
        
    def add_game(self, trajectory: List[Tuple[torch.Tensor, np.ndarray, int]], winner_id: int):
        """
        Add a full game trajectory to the buffer using the final outcome.
        
        Args:
            trajectory: List of (state, policy_probs, player_id)
            winner_id: The ID of the winning player (1 or 2). 0 for draw.
        """
        for state, probs, current_player in trajectory:
            if winner_id == 0:
                value = 0.0
            elif winner_id == current_player:
                value = 1.0
            else:
                value = -1.0
                
            self.add(state, probs, value)
            
    def sample(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Sample a batch of training data.
        
        Returns:
            (states, policies, values) stacked as tensors.
        """
        batch = random.sample(self.buffer, min(len(self.buffer), batch_size))
        
        states, policies, values = zip(*batch)
        
        # Stack into tensors
        state_batch = torch.stack(states)
        policy_batch = torch.tensor(np.stack(policies), dtype=torch.float32)
        value_batch = torch.tensor(values, dtype=torch.float32).unsqueeze(1)
        
        return state_batch, policy_batch, value_batch
    
    def __len__(self) -> int:
        return len(self.buffer)
